# Writing Effective Prompts for Lead Recovery

This guide explains how to write effective prompts for the Lead Recovery project's LLM component, ensuring that they work properly with the YAML output structure defined in `meta.yml`.

## Prompt Structure

An effective prompt for the Lead Recovery project should have these key components:

1. **Role Definition**: Define the role and expertise of the LLM
2. **Task Description**: Clearly state what analysis is needed
3. **Conversation Display**: Show where the conversation will be inserted
4. **Context Variables**: Include relevant variables from processors
5. **Output Format**: Specify the YAML structure that matches your `meta.yml`

## Basic Prompt Template

```
You are an expert at analyzing conversations between leads and Kuna chatbots. Your task is to analyze the following conversation and determine the current status and next steps.

CONVERSATION:
{conversation}

ADDITIONAL CONTEXT:
- Hours since last user message: {HOURS_MINUTES_SINCE_LAST_USER_MESSAGE}
- Handoff status: {handoff_finalized}
- Last message sender: {last_message_sender}
- Last user message: {last_user_message_text}

Based on the conversation above, provide your analysis in the following YAML format:

```yaml
summary: Brief summary of what the conversation is about
lead_status: ACTIVE | STALLED | CONVERTED
next_action: FOLLOW_UP | WAIT | CLOSE
next_action_reasoning: Brief explanation of your reasoning for the next action
```
```

## Matching Prompts with meta.yml

The YAML structure in your prompt **must** exactly match the `expected_llm_keys` in your `meta.yml` file. Here's how they should align:

### Prompt YAML Section
```
```yaml
summary: Brief summary of what the conversation is about
lead_status: ACTIVE | STALLED | CONVERTED
next_action: FOLLOW_UP | WAIT | CLOSE
next_action_reasoning: Brief explanation of your reasoning for the next action
```
```

### Corresponding meta.yml Section
```yaml
llm_config:
  prompt_file: "prompt.txt"
  expected_llm_keys:
    summary:
      description: "Brief summary of what the conversation is about"
      type: str
    lead_status:
      description: "Current status of the lead"
      type: str
      enum_values: ["ACTIVE", "STALLED", "CONVERTED"]
    next_action:
      description: "Recommended next action"
      type: str
      enum_values: ["FOLLOW_UP", "WAIT", "CLOSE"]
    next_action_reasoning:
      description: "Explanation for the next action recommendation"
      type: str
```

## Context Variables

You can inject variables from processors into your prompt using curly braces:

```
Hours since last user message: {HOURS_MINUTES_SINCE_LAST_USER_MESSAGE}
```

All keys generated by the active Python processors (as configured in `meta.yml`) are automatically made available as context variables within the LLM prompt template. You can use any of these generated keys (e.g., `{HOURS_MINUTES_SINCE_LAST_USER_MESSAGE}`, `{handoff_finalized}`) directly in your `prompt.txt`.

The `llm_config.context_keys_from_python` list in your `meta.yml` serves as a declaration or manifest of which of these available processor-generated keys you *intend* to use in your prompt. While not strictly enforced by the summarizer for filtering (all processor outputs are passed to the prompt), it's good practice to list them here for clarity, documentation, and potential future validation or tooling.

Example `meta.yml` section:
```yaml
llm_config:
  context_keys_from_python:
    - "HOURS_MINUTES_SINCE_LAST_USER_MESSAGE"
```

## Best Practices for Effective Prompts

### 1. Be Explicit About Output Format

Always specify the exact YAML structure you expect:

```
Provide your analysis in the following YAML format:

```yaml
key1: value1
key2: value2
```
```

### 2. Describe Enum Values

For fields with limited valid values, list all options:

```
status: ACTIVE | STALLED | CONVERTED
```

This helps the LLM provide consistent responses that match your expectations.

### 3. Include Clear Instructions

Tell the LLM exactly what to analyze and what to ignore:

```
Focus on identifying whether the customer needs assistance with billing or technical support. Ignore general inquiries about the product.
```

### 4. Use Processor Context Effectively

Prioritize the most relevant processor outputs to include in your prompt:

```
- Conversation state: {conversation_state}
- Template detected: {recovery_template_detected}
- Hours since last message: {HOURS_MINUTES_SINCE_LAST_MESSAGE}
```

### 5. Set Length Expectations

If you want concise or detailed outputs, be explicit:

```
Provide a brief summary (2-3 sentences maximum) of the conversation.
```

## Example Prompts

### Basic Lead Status Classification

```
You are analyzing customer conversations. Review the following conversation and determine the customer's status.

CONVERSATION:
{conversation}

CONTEXT:
- Days since last message: {HOURS_MINUTES_SINCE_LAST_MESSAGE}

Provide your analysis in YAML format:

```yaml
status: ACTIVE | DORMANT | CHURNED
reason: Brief explanation for this classification
priority: HIGH | MEDIUM | LOW
```
```

### Handoff Analysis Prompt

```
You are evaluating conversations for handoff quality. Review this conversation to determine if handoff was handled properly.

CONVERSATION:
{conversation}

CONTEXT:
- Handoff detected: {handoff_invitation_detected}
- Handoff finalized: {handoff_finalized}

Analyze the handoff process and provide your evaluation in YAML format:

```yaml
handoff_quality: EXCELLENT | GOOD | NEEDS_IMPROVEMENT | POOR
issues_identified: List any issues with the handoff process
improvement_suggestions: Suggestions for improving the handoff
```
```

## Troubleshooting Prompt Issues

If your LLM is not producing correctly formatted outputs:

1. **Verify format instructions**: Ensure you've clearly specified YAML format with proper formatting
2. **Check enum values**: Confirm all possible values are explicitly listed
3. **Simplify complex structures**: Break down complex nested structures into simpler formats
4. **Test with sample output**: Include a sample output in your prompt to demonstrate
5. **Review for ambiguity**: Ensure instructions are clear and not open to interpretation 